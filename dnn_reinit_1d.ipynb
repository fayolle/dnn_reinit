{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t7TlHimL0sSs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kRDpnral09Jv"
      },
      "outputs": [],
      "source": [
        "class Implicit(nn.Module):\n",
        "  def __init__(self, fun, dimension, radius=1):\n",
        "    super(Implicit, self).__init__()\n",
        "\n",
        "    d_in = dimension\n",
        "    dims = [512, 512, 512, 512, 512, 512, 512, 512]\n",
        "    beta = 100\n",
        "    skip_in = [4]\n",
        "    radius_init = radius\n",
        "\n",
        "    dims = [d_in] + dims + [1]\n",
        "\n",
        "    self.num_layers = len(dims)\n",
        "    self.skip_in = skip_in\n",
        "\n",
        "    for layer in range(0, self.num_layers - 1):\n",
        "\n",
        "      if layer + 1 in skip_in:\n",
        "        out_dim = dims[layer + 1] - d_in\n",
        "      else:\n",
        "        out_dim = dims[layer + 1]\n",
        "\n",
        "      lin = nn.Linear(dims[layer], out_dim)\n",
        "\n",
        "      if layer == self.num_layers - 2:\n",
        "        torch.nn.init.normal_(lin.weight, mean=np.sqrt(np.pi) / np.sqrt(dims[layer]), std=0.00001)\n",
        "        torch.nn.init.constant_(lin.bias, -radius_init)\n",
        "      else:\n",
        "        torch.nn.init.constant_(lin.bias, 0.0)\n",
        "        torch.nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n",
        "\n",
        "      setattr(self, \"lin\" + str(layer), lin)\n",
        "\n",
        "    self.activation = nn.Softplus(beta=beta)\n",
        "    #self.activation = nn.ReLU()\n",
        "    self.fun = fun\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = input\n",
        "    for layer in range(0, self.num_layers - 1):\n",
        "      lin = getattr(self, \"lin\" + str(layer))\n",
        "      if layer in self.skip_in:\n",
        "        x = torch.cat([x, input], -1) / np.sqrt(2)\n",
        "      x = lin(x)\n",
        "      if layer < self.num_layers - 2:\n",
        "        x = self.activation(x)\n",
        "    fun_sign = torch.tanh(0.1*self.fun(input))\n",
        "    fun_sign = fun_sign.reshape(x.shape)\n",
        "    x = x * fun_sign\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nGG90eQD1FWp"
      },
      "outputs": [],
      "source": [
        "def grad(y, x, grad_outputs=None):\n",
        "  if grad_outputs is None:\n",
        "    grad_outputs = torch.ones_like(y)\n",
        "  grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "  return grad\n",
        "\n",
        "def div(y, x):\n",
        "  div = 0.\n",
        "  for i in range(y.shape[-1]):\n",
        "    div += torch.autograd.grad(y[..., i], x, torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
        "  return div\n",
        "\n",
        "def laplacian(y, x):\n",
        "  grad = grad(y, x)\n",
        "  return div(grad, x)\n",
        "\n",
        "def pLaplacian(y, x, p=2):\n",
        "  g = grad(y, x)\n",
        "  #g_n = g.norm(2, dim=1)\n",
        "  g_n = torch.linalg.norm(g, 2, dim=1)\n",
        "  g_n = g_n**(p-2)\n",
        "  g_n = torch.reshape(g_n, (g.shape[0],1))\n",
        "  g = g_n * g\n",
        "  return div(g, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KVpbRmnN1KPc"
      },
      "outputs": [],
      "source": [
        "def uniform_data(num_points, device, xbounds):\n",
        "  xmin, xmax = xbounds\n",
        "  x = torch.FloatTensor(num_points, 1).uniform_(xmin, xmax)\n",
        "  return x.to(device)\n",
        "\n",
        "def grid_data(num_points, device, xbounds):\n",
        "  xmin, xmax = xbounds\n",
        "  x = torch.linspace(xmin, xmax, num_points, requires_grad=True, device=device)\n",
        "  xt = x.resize(x.shape[0],1)\n",
        "  return xt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jsRZxbme1SlO"
      },
      "outputs": [],
      "source": [
        "def trainPPoisson(num_iters, fun, xbounds, p=2, device='cpu'):\n",
        "  model = Implicit(fun=fun, dimension=1).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  n_samples = 256\n",
        "  loss = 0\n",
        "\n",
        "  # Train network\n",
        "  for i in range(0, num_iters):\n",
        "    # Uniform samples\n",
        "    u = uniform_data(n_samples, device, xbounds)\n",
        "    u.requires_grad = True\n",
        "    \n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    f_d = model(u)\n",
        "    \n",
        "    # p-Laplacian\n",
        "    lap = pLaplacian(f_d, u, p)\n",
        "    lap_loss = torch.mean((lap+1)**2)\n",
        "    loss = lap_loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    #clip_grad = 1.0\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
        "\n",
        "    optimizer.step()\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ig3YTg7SBCIb"
      },
      "outputs": [],
      "source": [
        "def trainEikonal(num_iters, fun, xbounds, device='cpu'):\n",
        "  model = Implicit(fun=fun, dimension=1).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  n_samples = 256\n",
        "  loss = 0\n",
        "\n",
        "  # Train network\n",
        "  for i in range(0, num_iters):\n",
        "    # Uniform samples\n",
        "    u = uniform_data(n_samples, device, xbounds)\n",
        "    u.requires_grad = True\n",
        "    \n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    f_d = model(u)\n",
        "    \n",
        "    g_d = grad(f_d, u)\n",
        "    g_norm = (g_d.norm(2, dim=1) - 1)**2\n",
        "    eik_loss = torch.mean(g_norm)\n",
        "    loss = eik_loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    #clip_grad = 1.0\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
        "\n",
        "    optimizer.step()\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SmM57zeO18WE"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  x1 = x\n",
        "  x2 = 1-x\n",
        "  return x1+x2-torch.sqrt(x1**2+x2**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vGYWh0L3AR41"
      },
      "outputs": [],
      "source": [
        "# Domain boundary\n",
        "xbounds = (-2.0, 3.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbbg-F9B1ZY-",
        "outputId": "d8d91bd9-49d9-4e58-8098-236111ac1092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIDPUSRL1g8K",
        "outputId": "ebf6a80c-a2e4-4401-ecde-a203c05c651b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/tensor.py:474: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ]
        }
      ],
      "source": [
        "x = torch.linspace(xbounds[0], xbounds[1], 1000, requires_grad=True, device=device)\n",
        "xt=x.resize(x.shape[0],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vQf_xKp65t8X"
      },
      "outputs": [],
      "source": [
        "yt = f(xt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "h5byAYHX50Qw",
        "outputId": "e5206484-745b-4ad3-c919-da0917d2f8a8"
      },
      "outputs": [],
      "source": [
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize=11)\n",
        "plt.rc('ytick', labelsize=11)\n",
        "fig = plt.figure(figsize=(8, 6), dpi=100)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(x.cpu().detach().numpy(), yt.cpu().detach().numpy(), label='R-functions')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DDCp2LUG1f9q"
      },
      "outputs": [],
      "source": [
        "# p-Poisson problem, p = 2\n",
        "max_iteration = 15000\n",
        "p = 2\n",
        "model_p2 = trainPPoisson(max_iteration, fun=f, xbounds=xbounds, p=p, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a9YF479A1v_J"
      },
      "outputs": [],
      "source": [
        "yt_p2 = model_p2(xt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "H2jDXWCc1f6f",
        "outputId": "fbbd8b45-56e0-4586-e08b-2f21df1214ac"
      },
      "outputs": [],
      "source": [
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize=11)\n",
        "plt.rc('ytick', labelsize=11)\n",
        "fig = plt.figure(figsize=(8, 6), dpi=100)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(x.cpu().detach().numpy(), yt_p2.cpu().detach().numpy(), label='p-Poisson, p=2')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "CXsFTy5usKE9",
        "outputId": "55fac13c-aaf9-4a2e-bbaa-a97983e8efda"
      },
      "outputs": [],
      "source": [
        "# When solving the p-Laplacian problem, check the distribution of the p-Laplacian\n",
        "print('p='+str(p))\n",
        "lap_p2 = pLaplacian(yt_p2, xt, p=p)\n",
        "print(str(lap_p2.min()) + ' ' + str(lap_p2.max()) + ' ' + str(lap_p2.mean()))\n",
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize=11)\n",
        "plt.rc('ytick', labelsize=11)\n",
        "fig = plt.figure(figsize=(8, 6), dpi=100)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.hist(lap_p2.cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DgbxEgcMSRRZ"
      },
      "outputs": [],
      "source": [
        "# p-Poisson problem, p = 8\n",
        "max_iteration = 15000\n",
        "p = 8\n",
        "model_p8 = trainPPoisson(max_iteration, fun=f, xbounds=xbounds, p=p, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a9lhVUUVSczF"
      },
      "outputs": [],
      "source": [
        "yt_p8 = model_p8(xt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "oGtSr7YaSm2A",
        "outputId": "10edeed0-7501-440b-a086-91103677e062"
      },
      "outputs": [],
      "source": [
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize=11)\n",
        "plt.rc('ytick', labelsize=11)\n",
        "fig = plt.figure(figsize=(8, 6), dpi=100)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(x.cpu().detach().numpy(), yt_p8.cpu().detach().numpy(), label='p-Poisson, p=8')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "AubgBLwMTJV_",
        "outputId": "a5ddd93a-1d0e-4089-b750-aba9f3afad71"
      },
      "outputs": [],
      "source": [
        "# When solving the p-Laplacian problem, check the distribution of the p-Laplacian\n",
        "print('p='+str(p))\n",
        "lap_8 = pLaplacian(yt_p8, xt, p=p)\n",
        "print(str(lap_8.min()) + ' ' + str(lap_8.max()) + ' ' + str(lap_8.mean()))\n",
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize=11)\n",
        "plt.rc('ytick', labelsize=11)\n",
        "fig = plt.figure(figsize=(8, 6), dpi=100)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.hist(lap_8.cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IWVVJPVhTa2m"
      },
      "outputs": [],
      "source": [
        "# Eikonal problem\n",
        "max_iteration = 15000\n",
        "model_eik = trainEikonal(max_iteration, fun=f, xbounds=xbounds, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-x4XulTMTnXn"
      },
      "outputs": [],
      "source": [
        "yt_eik = model_eik(xt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "FIYVGyDATv24",
        "outputId": "bbd1a8d4-9fb9-42af-9539-12d4ed0bab98"
      },
      "outputs": [],
      "source": [
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize=11)\n",
        "plt.rc('ytick', labelsize=11)\n",
        "fig = plt.figure(figsize=(8, 6), dpi=100)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(x.cpu().detach().numpy(), yt_eik.cpu().detach().numpy(), label='Eikonal')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "fjUZplSvT5gE",
        "outputId": "f865993f-1c09-40f7-cfce-0c8df8ef08a3"
      },
      "outputs": [],
      "source": [
        "# When solving the Eikonal equation, check the gradient\n",
        "g_yt_eik = grad(yt_eik, xt)\n",
        "g_yt_eik_norm = g_yt_eik.norm(2, dim=1)\n",
        "print(str(g_yt_eik_norm.min()) + ' ' + str(g_yt_eik_norm.max()) + ' ' + str(g_yt_eik_norm.mean()))\n",
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize=11)\n",
        "plt.rc('ytick', labelsize=11)\n",
        "fig = plt.figure(figsize=(8, 6), dpi=100)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.hist(g_yt_eik_norm.cpu().detach().numpy())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMw0LRzzMo5Nd1kT/VCTXBR",
      "name": "dnn_reinit_1d.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
