{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dnn_reinit_3d.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPUf2vMkrrTVzem9fdPC9g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqXU4Jeey9Y7"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK0pPVxozoMN"
      },
      "source": [
        "class Implicit(nn.Module):\n",
        "  def __init__(self, fun, dimension):\n",
        "    super(Implicit, self).__init__()\n",
        "\n",
        "    d_in = dimension\n",
        "    dims = [512, 512, 512, 512, 512, 512, 512, 512]\n",
        "    beta = 100\n",
        "    skip_in = [4]\n",
        "    radius_init = 1\n",
        "    dims = [d_in] + dims + [1]\n",
        "\n",
        "    self.num_layers = len(dims)\n",
        "    self.skip_in = skip_in\n",
        "\n",
        "    for layer in range(0, self.num_layers - 1):\n",
        "      if layer + 1 in skip_in:\n",
        "        out_dim = dims[layer + 1] - d_in\n",
        "      else:\n",
        "        out_dim = dims[layer + 1]\n",
        "      lin = nn.Linear(dims[layer], out_dim)\n",
        "      if layer == self.num_layers - 2:\n",
        "        torch.nn.init.normal_(lin.weight, mean=np.sqrt(np.pi) / np.sqrt(dims[layer]), std=0.00001)\n",
        "        torch.nn.init.constant_(lin.bias, -radius_init)\n",
        "      else:\n",
        "        torch.nn.init.constant_(lin.bias, 0.0)\n",
        "        torch.nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n",
        "      setattr(self, \"lin\" + str(layer), lin)\n",
        "    self.activation = nn.Softplus(beta=beta)\n",
        "    #self.activation = nn.ReLU()\n",
        "    self.fun = fun\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = input\n",
        "    for layer in range(0, self.num_layers - 1):\n",
        "      lin = getattr(self, \"lin\" + str(layer))\n",
        "      if layer in self.skip_in:\n",
        "        x = torch.cat([x, input], -1) / np.sqrt(2)\n",
        "      x = lin(x)\n",
        "      if layer < self.num_layers - 2:\n",
        "        x = self.activation(x)\n",
        "    fun_sign = torch.tanh(0.1*self.fun(input))\n",
        "    fun_sign = fun_sign.reshape(x.shape)\n",
        "    x = x * fun_sign\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0zHNewGz7KZ"
      },
      "source": [
        "def grad(y, x):\n",
        "  g = autograd.grad(y, [x], grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
        "  return g\n",
        "\n",
        "def div(y, x):\n",
        "  div = 0.0\n",
        "  for i in range(y.shape[-1]):\n",
        "    div += autograd.grad(y[..., i], x, grad_outputs=torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
        "  return div\n",
        "\n",
        "def Laplacian(y, x):\n",
        "  g = grad(y, x)\n",
        "  return div(g, x)\n",
        "\n",
        "def pLaplacian(y, x, p=2):\n",
        "  g = grad(y, x)\n",
        "  #g_n = g.norm(2, dim=1)\n",
        "  g_n = torch.linalg.norm(g, 2, dim=1)\n",
        "  g_n = g_n**(p-2)\n",
        "  g_n = torch.reshape(g_n, (g.shape[0],1))\n",
        "  g = g_n * g\n",
        "  return div(g, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCr9-xF-0Zz-"
      },
      "source": [
        "def uniformSamples(num_points, grid_min, grid_max, device):\n",
        "  xx = torch.FloatTensor(num_points, 1).uniform_(grid_min[0], grid_max[0])\n",
        "  yy = torch.FloatTensor(num_points, 1).uniform_(grid_min[1], grid_max[1])\n",
        "  zz = torch.FloatTensor(num_points, 1).uniform_(grid_min[2], grid_max[2])\n",
        "  x = torch.cat((xx,yy,zz), dim=-1)\n",
        "  return x.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BelAFnVg0g69"
      },
      "source": [
        "def trainPPoisson(num_iters, fun, grid_min, grid_max, p=2, device='cpu'):\n",
        "  assert(len(grid_min) == len(grid_max))\n",
        "  dimension = len(grid_min)\n",
        "\n",
        "  model = Implicit(fun=fun, dimension=dimension).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  n_samples = 1024\n",
        "  loss = 0\n",
        "\n",
        "  # regular Ggrid data\n",
        "  #res = (16,16,16)\n",
        "  #x = grid_samples(res, grid_min, grid_max, device)\n",
        "  #x.requires_grad = True\n",
        "\n",
        "  # Train network\n",
        "  for i in range(0, num_iters):\n",
        "    # Uniform samples\n",
        "    x = uniformSamples(n_samples, grid_min, grid_max, device)\n",
        "    x.requires_grad = True\n",
        "\n",
        "    # Input implicit \n",
        "    fun_d = fun(x)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    f_d = model(x)\n",
        "\n",
        "    # Laplacian \n",
        "    #lap = laplacian(f_d, coords_d)\n",
        "    #loss = torch.mean((lap + 1)**2)\n",
        "    # p-Laplacian\n",
        "    lap = pLaplacian(f_d, x, p)\n",
        "    lap_constraint = torch.mean((lap+1)**2)\n",
        "\n",
        "    # Penalize extra zeros\n",
        "    extra_constraint = torch.mean(torch.where(torch.abs(fun_d)<1e-3, torch.zeros_like(f_d), torch.exp(-1e2 * torch.abs(f_d))))\n",
        "\n",
        "    loss = lap_constraint + extra_constraint\n",
        "\n",
        "    #if i%500 == 0:\n",
        "    #print(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    #clip_grad = 1.0\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
        "    optimizer.step()\n",
        "\n",
        "  # model.eval()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCwRpqhYbzVT"
      },
      "source": [
        "def trainEikonal(num_iters, fun, grid_min, grid_max, device='cpu'):\n",
        "  assert(len(grid_min) == len(grid_max))\n",
        "  dimension = len(grid_min)\n",
        "\n",
        "  model = Implicit(fun=fun, dimension=dimension).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  n_samples = 1024\n",
        "  loss = 0\n",
        "\n",
        "  # regular Ggrid data\n",
        "  #res = (16,16,16)\n",
        "  #x = grid_samples(res, grid_min, grid_max, device)\n",
        "  #x.requires_grad = True\n",
        "\n",
        "  # Train network\n",
        "  for i in range(0, num_iters):\n",
        "    # Uniform samples\n",
        "    x = uniformSamples(n_samples, grid_min, grid_max, device)\n",
        "    x.requires_grad = True\n",
        "\n",
        "    # Input implicit \n",
        "    fun_d = fun(x)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    f_d = model(x)\n",
        "\n",
        "    g_d = grad(f_d, x)\n",
        "    g_norm = (g_d.norm(2, dim=1) - 1)**2\n",
        "    g_constraint = torch.mean(g_norm)\n",
        "\n",
        "    # Penalize extra zeros\n",
        "    extra_constraint = torch.mean(torch.where(torch.abs(fun_d)<1e-3, torch.zeros_like(f_d), torch.exp(-1e2 * torch.abs(f_d))))\n",
        "\n",
        "    loss = g_constraint + extra_constraint\n",
        "\n",
        "    #if i%500 == 0:\n",
        "    #print(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    #clip_grad = 1.0\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
        "    optimizer.step()\n",
        "\n",
        "  # model.eval()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM8DhFG31d91"
      },
      "source": [
        "def saveVTK(filename, xyz, res, field):\n",
        "  resx, resy, resz = res\n",
        "    \n",
        "  field_title = 'VALUE'\n",
        "\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write('# vtk DataFile Version 3.0\\n')\n",
        "    f.write('vtk output\\n')\n",
        "    f.write('ASCII\\n')\n",
        "    f.write('DATASET STRUCTURED_GRID\\n')\n",
        "    f.write('DIMENSIONS ' + str(resx) + ' ' + str(resy) + ' ' + str(resz) +'\\n')\n",
        "    f.write('POINTS ' + str(resx*resy*resz) + ' double\\n')\n",
        "\n",
        "    if (torch.is_tensor(xyz)):\n",
        "      np.savetxt(f, xyz.detach().cpu().numpy())\n",
        "    else:\n",
        "      np.savetxt(f, xyz)\n",
        "    \n",
        "    f.write('\\n\\n')\n",
        "\n",
        "    f.write('POINT_DATA ' + str(resx*resy*resz) + '\\n')\n",
        "    f.write('SCALARS ' + field_title + ' double' + '\\n')\n",
        "    f.write('LOOKUP_TABLE default\\n')\n",
        "        \n",
        "    if (torch.is_tensor(field)):\n",
        "      np.savetxt(f, field.detach().cpu().numpy())\n",
        "    else:\n",
        "      np.savetxt(f, field)\n",
        "    f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS2NaF12el6m"
      },
      "source": [
        "def torchLinearGrid(grid_min, grid_max, grid_res, device='cpu'):\n",
        "  resx, resy, resz = grid_res\n",
        "  dx = grid_max[0]-grid_min[0]\n",
        "  x = torch.arange(grid_min[0], grid_max[0], step=dx/float(resx))\n",
        "  dy = grid_max[1]-grid_min[1]\n",
        "  y = torch.arange(grid_min[1], grid_max[1], step=dy/float(resy))\n",
        "  dz = grid_max[2]-grid_min[2]\n",
        "  z = torch.arange(grid_min[2], grid_max[2], step=dz/float(resz))\n",
        "  xx, yy, zz = torch.meshgrid(x, y, z)\n",
        "  xx = xx.to(device)\n",
        "  yy = yy.to(device)\n",
        "  zz = zz.to(device)\n",
        "  dimg = resx * resy * resz\n",
        "  xyz = torch.stack((xx, yy, zz), dim=-1).reshape(dimg,3)\n",
        "  return xyz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuy7OOq2ewzR"
      },
      "source": [
        "def torchLinearSampling(model, xyz):\n",
        "  d = model(xyz)\n",
        "  return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5ArYiMW31cJ"
      },
      "source": [
        "# Primitives and operations\n",
        "def sphere(p, center, r):\n",
        "  x0 = p[:,0] - center[0]\n",
        "  x1 = p[:,1] - center[1]\n",
        "  x2 = p[:,2] - center[2]\n",
        "  return r**2 - x0**2 - x1**2 - x2**2\n",
        "\n",
        "def cylX(p, center, r):\n",
        "  x1 = p[:,1] - center[1]\n",
        "  x2 = p[:,2] - center[2]\n",
        "  return r**2 - x1**2 - x2**2\n",
        "\n",
        "def cylY(p, center, r):\n",
        "  x0 = p[:,0] - center[0]\n",
        "  x2 = p[:,2] - center[2]\n",
        "  return r**2 - x0**2 - x2**2\n",
        "\n",
        "def cylZ(p, center, r):\n",
        "  x0 = p[:,0] - center[0]\n",
        "  x1 = p[:,1] - center[1]\n",
        "  return r**2 - x0**2 - x1**2\n",
        "\n",
        "def block(p, vertex, dx, dy, dz):\n",
        "  x0 = -(p[:,0]-vertex[0]) * (p[:,0]-(vertex[0]+dx))\n",
        "  x1 = -(p[:,1]-vertex[1]) * (p[:,1]-(vertex[1]+dy))\n",
        "  x2 = -(p[:,2]-vertex[2]) * (p[:,2]-(vertex[2]+dz))\n",
        "  t0 = x0 + x1 - torch.sqrt(x0**2 + x1**2)\n",
        "  return t0 + x2 - torch.sqrt(t0**2 + x2**2)\n",
        "\n",
        "def difference(d1, d2):\n",
        "  d = d1 - d2 - torch.sqrt(d1**2 + d2**2)\n",
        "  return d\n",
        "\n",
        "def intersection(d1, d2):\n",
        "  d = d1 + d2 - torch.sqrt(d1**2 + d2**2)\n",
        "  return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OYhgv8v37oW"
      },
      "source": [
        "def model(p):\n",
        "  sp1 = sphere(p, center=(0,0,0), r=1)\n",
        "  b1 = block(p, vertex=(-0.75, -0.75, -0.75), dx=1.5, dy=1.5, dz=1.5)\n",
        "  t1 = intersection(sp1, b1)\n",
        "  c1 = cylX(p, center=(0,0,0), r=0.5)\n",
        "  c2 = cylY(p, center=(0,0,0), r=0.5)\n",
        "  c3 = cylZ(p, center=(0,0,0), r=0.5)\n",
        "  t2 = difference(t1, c1)\n",
        "  t3 = difference(t2, c2)\n",
        "  t4 = difference(t3, c3)\n",
        "  return t4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbn9l9FV4BUw",
        "outputId": "74cf24dd-89fa-4ad7-bd90-600500850e84"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1GCXrsEjwN5"
      },
      "source": [
        "xyz = torchLinearGrid(grid_min=(-1.5,-1.5,-1.5), grid_max=(1.5,1.5,1.5), grid_res=(64,64,64), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVlQVpXQjsoZ"
      },
      "source": [
        "# Original model\n",
        "f_orig_xyz = model(xyz)\n",
        "saveVTK('3d_model_orig.vtk', xyz, (64,64,64), f_orig_xyz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW1u8Qcc3-GR"
      },
      "source": [
        "# p-Poisson\n",
        "#model_p8 = trainPPoisson(num_iters=10000, fun=model, grid_min=(-1.5,-1.5,-1.5), grid_max=(1.5,1.5,1.5), p=8, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfWQA8wNcyKX"
      },
      "source": [
        "#f_p8_xyz = torchLinearSampling(model_p8, xyz)\n",
        "#saveVTK('3d_model_pPoisson_p8.vtk', xyz, (64,64,64), f_p8_xyz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzGYBKB1MlCj"
      },
      "source": [
        "# Eikonal\n",
        "model_eik = trainEikonal(num_iters=10000, fun=model, grid_min=(-1.5,-1.5,-1.5), grid_max=(1.5,1.5,1.5), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogMfWS5KMy_i"
      },
      "source": [
        "f_eik_xyz = torchLinearSampling(model_eik, xyz)\n",
        "saveVTK('3d_model_eikonal.vtk', xyz, (64,64,64), f_eik_xyz)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}